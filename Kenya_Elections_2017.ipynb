{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import subprocess\n",
    "\n",
    "import json\n",
    "import csv\n",
    "\n",
    "import uuid\n",
    "\n",
    "from IPython.display import display_javascript, display_html, display\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from datetime import datetime, date, time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.10.1\n"
     ]
    }
   ],
   "source": [
    "from platform import python_version\n",
    "print(python_version())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting git+https://github.com/JustAnotherArchivist/snscrape.git\n",
      "  Cloning https://github.com/JustAnotherArchivist/snscrape.git to c:\\users\\hp\\appdata\\local\\temp\\pip-req-build-3simuean\n",
      "  Resolved https://github.com/JustAnotherArchivist/snscrape.git to commit d72b51953f0ec05ee18761ea31c1bb82f886f7a9\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Requirement already satisfied: requests[socks] in c:\\users\\hp\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from snscrape==0.4.3.20220107.dev56+gd72b519) (2.28.1)\n",
      "Requirement already satisfied: lxml in c:\\users\\hp\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from snscrape==0.4.3.20220107.dev56+gd72b519) (4.6.4)\n",
      "Requirement already satisfied: beautifulsoup4 in c:\\users\\hp\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from snscrape==0.4.3.20220107.dev56+gd72b519) (4.10.0)\n",
      "Requirement already satisfied: filelock in c:\\users\\hp\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from snscrape==0.4.3.20220107.dev56+gd72b519) (3.4.2)\n",
      "Requirement already satisfied: soupsieve>1.2 in c:\\users\\hp\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from beautifulsoup4->snscrape==0.4.3.20220107.dev56+gd72b519) (2.3.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\hp\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests[socks]->snscrape==0.4.3.20220107.dev56+gd72b519) (3.3)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\hp\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests[socks]->snscrape==0.4.3.20220107.dev56+gd72b519) (1.26.7)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in c:\\users\\hp\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests[socks]->snscrape==0.4.3.20220107.dev56+gd72b519) (2.0.9)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\hp\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests[socks]->snscrape==0.4.3.20220107.dev56+gd72b519) (2021.10.8)\n",
      "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in c:\\users\\hp\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests[socks]->snscrape==0.4.3.20220107.dev56+gd72b519) (1.7.1)\n",
      "Using legacy 'setup.py install' for snscrape, since package 'wheel' is not installed.\n",
      "Installing collected packages: snscrape\n",
      "  Attempting uninstall: snscrape\n",
      "    Found existing installation: snscrape 0.4.3.20220107.dev8+g7f88678\n",
      "    Uninstalling snscrape-0.4.3.20220107.dev8+g7f88678:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  Running command git clone --filter=blob:none --quiet https://github.com/JustAnotherArchivist/snscrape.git 'C:\\Users\\hp\\AppData\\Local\\Temp\\pip-req-build-3simuean'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Successfully uninstalled snscrape-0.4.3.20220107.dev8+g7f88678\n",
      "  Running setup.py install for snscrape: started\n",
      "  Running setup.py install for snscrape: finished with status 'done'\n",
      "Successfully installed snscrape-0.4.3.20220107.dev56+gd72b519\n",
      "\n",
      "[notice] A new release of pip available: 22.1.2 -> 22.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install git+https://github.com/JustAnotherArchivist/snscrape.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import snscrape.modules.twitter as sntwitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "usage: snscrape [-h] [--version] [--citation] [-v] [--dump-locals] [--retry N]\n",
      "                [-n N] [-f FORMAT | --jsonl] [--with-entity]\n",
      "                [--since DATETIME] [--progress]\n",
      "                SCRAPER ...\n",
      "\n",
      "options:\n",
      "  -h, --help            show this help message and exit\n",
      "  --version             show program's version number and exit\n",
      "  --citation            Display recommended citation information and exit\n",
      "                        (default: None)\n",
      "  -v, --verbose, --verbosity\n",
      "                        Increase output verbosity (default: 0)\n",
      "  --dump-locals         Dump local variables on serious log messages (warnings\n",
      "                        or higher) (default: False)\n",
      "  --retry N, --retries N\n",
      "                        When the connection fails or the server returns an\n",
      "                        unexpected response, retry up to N times with an\n",
      "                        exponential backoff (default: 3)\n",
      "  -n N, --max-results N\n",
      "                        Only return the first N results (default: None)\n",
      "  -f FORMAT, --format FORMAT\n",
      "                        Output format (default: None)\n",
      "  --jsonl               Output JSONL (default: False)\n",
      "  --with-entity         Include the entity (e.g. user, channel) as the first\n",
      "                        output item (default: False)\n",
      "  --since DATETIME      Only return results newer than DATETIME (default:\n",
      "                        None)\n",
      "  --progress            Report progress on stderr (default: False)\n",
      "\n",
      "scrapers:\n",
      "  SCRAPER\n",
      "    facebook-community\n",
      "    facebook-group\n",
      "    facebook-user\n",
      "    instagram-hashtag\n",
      "    instagram-location\n",
      "    instagram-user\n",
      "    reddit-search\n",
      "    reddit-subreddit\n",
      "    reddit-user\n",
      "    telegram-channel\n",
      "    twitter-hashtag\n",
      "    twitter-list-posts\n",
      "    twitter-profile\n",
      "    twitter-search\n",
      "    twitter-trends\n",
      "    twitter-tweet\n",
      "    twitter-user\n",
      "    vkontakte-user\n",
      "    weibo-user\n",
      "\n"
     ]
    }
   ],
   "source": [
    "output = subprocess.check_output(cmd, shell=True)\n",
    "                                 \n",
    "print(output.decode(\"utf-8\"))   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "in_folder = r\"C:\\\\Users\\\\hp\\\\Dropbox\\\\PC\\Documents\\\\others\\\\JR\\data\"\n",
    "\n",
    "out_folder = r\"C:\\\\Users\\\\hp\\\\Dropbox\\\\PC\\Documents\\\\others\\\\JR\\data\"\n",
    "\n",
    "\n",
    "json_filename = out_folder + '/RailaOdinga-query-tweets.json'\n",
    "\n",
    "#Using the OS library to call CLI commands in Python\n",
    "os.system(f'snscrape --max-results 1000 --jsonl --progress --since 2017-07-25 twitter-search \"#ElectionKenya2017 lang:en until:2017-08-22\" > {json_filename}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = date(2017, 7, 25)\n",
    "start = start.strftime('%Y-%m-%d')\n",
    "\n",
    "stop = date(2017, 8, 22)\n",
    "stop = stop.strftime('%Y-%m-%d')\n",
    "\n",
    "\n",
    "keyword = 'ElectionKenya2017'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "maxTweets = 1000\n",
    "\n",
    "#We are going to write the data into a csv file\n",
    "filename = out_folder + '/' + keyword + start + '-' + stop + '.csv'\n",
    "csvFile = open(filename, 'a', newline='', encoding='utf8')\n",
    "\n",
    "#We write to the csv file by using csv writer\n",
    "csvWriter = csv.writer(csvFile)\n",
    "csvWriter.writerow(['id','date','tweet', 'replies'])\n",
    "\n",
    "#I will use the following Twitter search operators:\n",
    "# since - start date for Tweets collection \n",
    "# stop  - stop date for Tweets collection\n",
    "# -filter:links - not very clear what this does, from Twitter search operators documentation: https://developer.twitter.com/en/docs/twitter-api/v1/rules-and-filtering/search-operators\n",
    "#                 but it looks like this will exclude tweets with links\n",
    "##from the search results\n",
    "# -filter:replies - removes @reply tweets from search results\n",
    "for i,tweet in enumerate(sntwitter.TwitterSearchScraper(keyword + 'since:' +  start + ' until:' + \\\n",
    "                                                        stop + ' -filter:links -filter:replies').get_items()):\n",
    "    if i > maxTweets :\n",
    "        break\n",
    "    csvWriter.writerow([tweet.id, tweet.date, tweet.content])\n",
    "\n",
    "csvFile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<_io.TextIOWrapper name='C:\\\\\\\\Users\\\\\\\\hp\\\\\\\\Dropbox\\\\\\\\PC\\\\Documents\\\\\\\\others\\\\\\\\JR\\\\files_data/ElectionsKE20172017-07-25-2017-08-22.csv' mode='a' encoding='utf8'>\n"
     ]
    }
   ],
   "source": [
    "print(csvFile)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.1 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  },
  "vscode": {
   "interpreter": {
    "hash": "63963b3f4c440940f0b94a3100916033a226cb4f45979123153792d60aa56d6a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import subprocess\n",
    "\n",
    "import json\n",
    "import csv\n",
    "\n",
    "import uuid\n",
    "\n",
    "from IPython.display import display_javascript, display_html, display\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from datetime import datetime, date, time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pandas\n",
      "  Downloading pandas-1.3.4-cp310-cp310-win_amd64.whl (10.2 MB)\n",
      "Collecting numpy>=1.21.0\n",
      "  Downloading numpy-1.21.4-cp310-cp310-win_amd64.whl (14.0 MB)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in c:\\users\\hp\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from pandas) (2.8.2)\n",
      "Collecting pytz>=2017.3\n",
      "  Downloading pytz-2021.3-py2.py3-none-any.whl (503 kB)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\hp\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from python-dateutil>=2.7.3->pandas) (1.16.0)\n",
      "Installing collected packages: pytz, numpy, pandas\n",
      "Successfully installed numpy-1.21.4 pandas-1.3.4 pytz-2021.3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: You are using pip version 21.2.4; however, version 21.3.1 is available.\n",
      "You should consider upgrading via the 'C:\\Users\\hp\\AppData\\Local\\Programs\\Python\\Python310\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    }
   ],
   "source": [
    "!pip install pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.10.1\n"
     ]
    }
   ],
   "source": [
    "from platform import python_version\n",
    "print(python_version())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting git+https://github.com/JustAnotherArchivist/snscrape.git\n",
      "  Cloning https://github.com/JustAnotherArchivist/snscrape.git to c:\\users\\hp\\appdata\\local\\temp\\pip-req-build-higj63ec\n",
      "  Resolved https://github.com/JustAnotherArchivist/snscrape.git to commit bcaa477b3d0fd8942dff71ac7a52d6a91ed2b008\n",
      "Collecting requests[socks]\n",
      "  Downloading requests-2.26.0-py2.py3-none-any.whl (62 kB)\n",
      "Collecting lxml\n",
      "  Downloading lxml-4.6.4-cp310-cp310-win_amd64.whl (3.5 MB)\n",
      "Collecting beautifulsoup4\n",
      "  Downloading beautifulsoup4-4.10.0-py3-none-any.whl (97 kB)\n",
      "Collecting soupsieve>1.2\n",
      "  Downloading soupsieve-2.3.1-py3-none-any.whl (37 kB)\n",
      "Collecting idna<4,>=2.5\n",
      "  Downloading idna-3.3-py3-none-any.whl (61 kB)\n",
      "Collecting urllib3<1.27,>=1.21.1\n",
      "  Downloading urllib3-1.26.7-py2.py3-none-any.whl (138 kB)\n",
      "Collecting certifi>=2017.4.17\n",
      "  Downloading certifi-2021.10.8-py2.py3-none-any.whl (149 kB)\n",
      "Collecting charset-normalizer~=2.0.0\n",
      "  Downloading charset_normalizer-2.0.9-py3-none-any.whl (39 kB)\n",
      "Collecting PySocks!=1.5.7,>=1.5.6\n",
      "  Downloading PySocks-1.7.1-py3-none-any.whl (16 kB)\n",
      "Using legacy 'setup.py install' for snscrape, since package 'wheel' is not installed.\n",
      "Installing collected packages: urllib3, idna, charset-normalizer, certifi, soupsieve, requests, PySocks, lxml, beautifulsoup4, snscrape\n",
      "    Running setup.py install for snscrape: started\n",
      "    Running setup.py install for snscrape: finished with status 'done'\n",
      "Successfully installed PySocks-1.7.1 beautifulsoup4-4.10.0 certifi-2021.10.8 charset-normalizer-2.0.9 idna-3.3 lxml-4.6.4 requests-2.26.0 snscrape-0.4.0.20211208 soupsieve-2.3.1 urllib3-1.26.7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  Running command git clone -q https://github.com/JustAnotherArchivist/snscrape.git 'C:\\Users\\hp\\AppData\\Local\\Temp\\pip-req-build-higj63ec'\n",
      "WARNING: You are using pip version 21.2.4; however, version 21.3.1 is available.\n",
      "You should consider upgrading via the 'C:\\Users\\hp\\AppData\\Local\\Programs\\Python\\Python310\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    }
   ],
   "source": [
    "!pip install git+https://github.com/JustAnotherArchivist/snscrape.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import snscrape.modules.twitter as sntwitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "cmd = 'snscrape --help'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "usage: snscrape [-h] [--version] [--citation] [-v] [--dump-locals] [--retry N]\n",
      "                [-n N] [-f FORMAT | --jsonl] [--with-entity]\n",
      "                [--since DATETIME] [--progress]\n",
      "                SCRAPER ...\n",
      "\n",
      "options:\n",
      "  -h, --help            show this help message and exit\n",
      "  --version             show program's version number and exit\n",
      "  --citation            Display recommended citation information and exit\n",
      "                        (default: None)\n",
      "  -v, --verbose, --verbosity\n",
      "                        Increase output verbosity (default: 0)\n",
      "  --dump-locals         Dump local variables on serious log messages (warnings\n",
      "                        or higher) (default: False)\n",
      "  --retry N, --retries N\n",
      "                        When the connection fails or the server returns an\n",
      "                        unexpected response, retry up to N times with an\n",
      "                        exponential backoff (default: 3)\n",
      "  -n N, --max-results N\n",
      "                        Only return the first N results (default: None)\n",
      "  -f FORMAT, --format FORMAT\n",
      "                        Output format (default: None)\n",
      "  --jsonl               Output JSONL (default: False)\n",
      "  --with-entity         Include the entity (e.g. user, channel) as the first\n",
      "                        output item (default: False)\n",
      "  --since DATETIME      Only return results newer than DATETIME (default:\n",
      "                        None)\n",
      "  --progress            Report progress on stderr (default: False)\n",
      "\n",
      "scrapers:\n",
      "  SCRAPER\n",
      "    facebook-community\n",
      "    facebook-group\n",
      "    facebook-user\n",
      "    instagram-hashtag\n",
      "    instagram-location\n",
      "    instagram-user\n",
      "    reddit-search\n",
      "    reddit-subreddit\n",
      "    reddit-user\n",
      "    telegram-channel\n",
      "    twitter-hashtag\n",
      "    twitter-list-posts\n",
      "    twitter-profile\n",
      "    twitter-search\n",
      "    twitter-trends\n",
      "    twitter-tweet\n",
      "    twitter-user\n",
      "    vkontakte-user\n",
      "    weibo-user\n",
      "\n"
     ]
    }
   ],
   "source": [
    "output = subprocess.check_output(cmd, shell=True)\n",
    "                                 \n",
    "print(output.decode(\"utf-8\"))   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "in_folder = r\"C:\\\\Users\\\\hp\\\\Dropbox\\\\PC\\Documents\\\\others\\\\JR\\data\"\n",
    "\n",
    "out_folder = r\"C:\\\\Users\\\\hp\\\\Dropbox\\\\PC\\Documents\\\\others\\\\JR\\data\"\n",
    "\n",
    "\n",
    "json_filename = out_folder + '/RailaOdinga-query-tweets.json'\n",
    "\n",
    "#Using the OS library to call CLI commands in Python\n",
    "os.system(f'snscrape --max-results 1000 --jsonl --progress --since 2017-07-25 twitter-search \"#ElectionKenya2017 lang:en until:2017-08-22\" > {json_filename}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = date(2017, 7, 25)\n",
    "start = start.strftime('%Y-%m-%d')\n",
    "\n",
    "stop = date(2017, 8, 22)\n",
    "stop = stop.strftime('%Y-%m-%d')\n",
    "\n",
    "\n",
    "keyword = 'ElectionKenya2017'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "maxTweets = 1000\n",
    "\n",
    "#We are going to write the data into a csv file\n",
    "filename = out_folder + '/' + keyword + start + '-' + stop + '.csv'\n",
    "csvFile = open(filename, 'a', newline='', encoding='utf8')\n",
    "\n",
    "#We write to the csv file by using csv writer\n",
    "csvWriter = csv.writer(csvFile)\n",
    "csvWriter.writerow(['id','date','tweet', 'replies'])\n",
    "\n",
    "#I will use the following Twitter search operators:\n",
    "# since - start date for Tweets collection \n",
    "# stop  - stop date for Tweets collection\n",
    "# -filter:links - not very clear what this does, from Twitter search operators documentation: https://developer.twitter.com/en/docs/twitter-api/v1/rules-and-filtering/search-operators\n",
    "#                 but it looks like this will exclude tweets with links\n",
    "##from the search results\n",
    "# -filter:replies - removes @reply tweets from search results\n",
    "for i,tweet in enumerate(sntwitter.TwitterSearchScraper(keyword + 'since:' +  start + ' until:' + \\\n",
    "                                                        stop + ' -filter:links -filter:replies').get_items()):\n",
    "    if i > maxTweets :\n",
    "        break\n",
    "    csvWriter.writerow([tweet.id, tweet.date, tweet.content])\n",
    "\n",
    "csvFile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<_io.TextIOWrapper name='C:\\\\\\\\Users\\\\\\\\hp\\\\\\\\Dropbox\\\\\\\\PC\\\\Documents\\\\\\\\others\\\\\\\\JR\\\\files_data/ElectionsKE20172017-07-25-2017-08-22.csv' mode='a' encoding='utf8'>\n"
     ]
    }
   ],
   "source": [
    "print(csvFile)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "a25b2d42e0fe57251b6ece2b75d30429fe26895efea1faac6949f166a7477be7"
  },
  "kernelspec": {
   "display_name": "Python 3.10.1 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
